---
title: "Reproduce simulation results"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Reproduce simulation results}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%",
  dpi = 300
)
```

NOTE: This is not included as a vignette with the package due to
long-running scripts. Instead, it will only be available as an article
on this website. This is intended to reproduce simulations shown in our
recent work ["Estimating single cell clonal dynamics in human blood
using coalescent
theory"](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/).
We'll skip over many of the basics, as those are detailed in our
introductory simulation article/vignette which is available in
`vignette("cloneRate-simulate")`. The simulation procedure for
generating birth-death trees is a direct implementation of results by
Amaury Lambert in [a recent
paper](https://pubmed.ncbi.nlm.nih.gov/29704514/). These results allowed
us to simulate thousands of trees to check our methods for growth rate
estimation from phylogenetic tree reconstruction, which is detailed in
[our recent
preprint.](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/)
We'll reproduce this validation here.

## Setup {.unnumbered}

First, we'll have to load the packages we want to use. We'll be plotting
the trees using the [`ape`](https://rdrr.io/cran/ape/) package function
[`ape::plot.phylo()`](https://rdrr.io/cran/ape/man/plot.phylo.html)
along with some other `ape` functions. If you have `cloneRate`
installed, you already have the
[`ape package`](https://rdrr.io/cran/ape/). We'll also be using
[`ggplot2`](https://ggplot2.tidyverse.org/) to make our plots, which can
be installed from CRAN as shown below:

```{r setup, results=FALSE, message = FALSE}
# Load and attach our package cloneRate
library(cloneRate)

# Load and attach ape, which will be installed if you've installed cloneRate
library(ape)

# Load and attach rstan, which is necessary for running MCMC's (birthDeathMCMC() and Phylofit)
library("rstan")

# Install ggplot2 if necessary, then load and attach it with library()
if (!requireNamespace("ggplot2")) {
  install.packages("ggplot2")
}
library(ggplot2)
```

We'll also set the color palette which we'll use for most of the
plotting. The palette is taken from
[here](https://davidmathlogic.com/colorblind/#%23000000-%23E69F00-%2356B4E9-%23009E73-%23F0E442-%230072B2-%23D55E00-%23CC79A7)

```{r setColors}
colorPal <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

## Running Phylofit

For applying Phylofit, which was developed by Nicholas Williams and
applied in [Williams et
al.](https://www.nature.com/articles/s41586-021-04312-6) and [Mitchell
et al.](https://www.nature.com/articles/s41586-022-04786-y), see Emily
Mitchell's github:
<https://github.com/emily-mitchell/normal_haematopoiesis/blob/main/1_functions/farm_only/phylofit.R>.
Here, we'll include the code from Emily Mithcell's github in a slightly
simplified form, as we don't want to apply the Aberrant Cell Fraction
(ACF) implementation of Phylofit, for reasons detailed in [our
paper](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/).
They implement Phylofit in Rstan, in a very similar manner to our
birth-death MCMC. First, we compile the Stan code:

```{r stanCode}
stan.code <- "functions{
real glogistic_lpdf(real[] t, real S, real tm, real T,real N){
int n;
real ll;
n=size(t);
ll=0.0;
for(k in 2:n){
ll=ll+log(1+exp(S*(t[k-1]-T+tm)));
ll=ll-(1/(S*N))*choose(k,2)*exp(-S*(T-tm-t[k]))*(exp(S*(t[k-1]-t[k]))-1);
ll=ll+choose(k,2)*(t[k]-t[k-1])/N+log(choose(k,2))-log(N);
}
return(ll);
}
}
data{
int N;      //number of tips in the tree
real t[N];  //timing of coalescences (with an added, unused zero)
real T;
real maxT;
real minT;
real maxLN;
real minLN;
}
parameters {
real<lower=0.0001,upper=4> S; //instantaneous growth rate-> exp(S)-1 per Year.
real <lower=minT,upper=maxT> tm; //midpoint
real<lower=minLN,upper=maxLN> LN; //log (base 10) pop size (i.e. carrying capacity)
}
model {
S ~ uniform(0.001,4);
tm ~ uniform(minT,maxT);
LN ~ uniform(minLN,maxLN);
t ~ glogistic(S,tm,T,pow(10,LN));
}
"
PHYLOFIT_STAN_MODEL <- stan_model(model_code=stan.code, model_name = "Phylofit")
```

Now that the Stan code is compiled, it's ready to be run. We'll wrap it
in a function similar to our other functions for estimating the growth
rate. We've made some cosmetic changes, but nothing that will affect the
Phylofit (no ACF) output for the estimated growth rate:

```{r phylofit}
# Note that this will only work for binary trees
phylofit <- function(ultratree, nchain = 3, nCores = 1, minLN=4, maxLN=7){
  
  # Get data from the tree
  coal_times <- sort(ape::branching.times(ultratree), decreasing = T)
  dat <- list("N" = ape::Ntip(ultratree), 
              "t" = as.numeric(c(coal_times, 0)), 
              "T" = max(coal_times), 
              "maxT" = 2*max(coal_times),
              "minLN" = minLN, "maxLN" = maxLN)
  dat$minT=as.numeric(dat$T-dat$t[1])
  
  # Run the Stan code
  stanr=sampling(PHYLOFIT_STAN_MODEL,
                 data=dat,iter = 20000,control=list(adapt_delta=0.95),
                 chains = nchain, cores = nCores)
  
  # Return the results
  ptile <- c(0.025, .5, .975)
  posterior <- quantile(rstan::extract(stanr)$S, ptile)
  data.frame("r_lb" = posterior[1], "estimate" = posterior[2],
             "r_ub" = posterior[3])
}
```

# Varying parameters

So far, we've looked at the same parameters. Now, we'll see what happens
when we change some important parameters about the birth-death process.

## Varying n

One of the most interesting results to explore further is how the
accuracy of the estimates depends on the number of samples. For this,
we'll look at a few different `n` parameter values. We can run
`simUltra()` with a vector input for `n`. First, we'll generate the
vector, which we'll call `n_vec`. Keeping with the paper, we'll produce
500 trees for each value of n:

```{r gen_n_vec}
n_vec <- rep(c(10, 30, 50, 100, 200, 500), each = 500)
```

We keep the growth rate `a - b` equal to 0.5, but let the actual birth
and death rates vary.

```{r a_vec}
a_vec <- stats::runif(n = length(n_vec), min = 0.5, max = 1.5)
b_vec <- a_vec - 0.5
```

In total, our `n_vec` is of length 3000. So we'll want to make sure
`a_vec` and `b_vec` are also of length 3000. And we'll have to set
`nTrees = 3000`. If we generate multiple trees, we must either set a
single value for the parameters, or a vector of length `nTrees`. This
applies to both `simUltra()` and `simMut()`. Let's generate some more
trees. Note: this will take a few minutes, but can be parallelized by
setting nCores \> 1.

```{r vary_n}
vary_n_trees <- simUltra(
  a = a_vec, b = b_vec, cloneAge = 40, n = n_vec,
  nTrees = length(n_vec), nCores = 6
)
```

Now, let's apply our estimates and combine the data.frames:

```{r mergeOutput}
# Apply our estimates for ultrametric trees
vary_n_maxLike <- suppressWarnings(maxLikelihood(vary_n_trees))
vary_n_lengths <- internalLengths(vary_n_trees)

# Combine the estimates
vary_n_analytical <- rbind(vary_n_maxLike, vary_n_lengths)
```

Let's make a plot for each of these, showing the performance. We'll use
`ggplot2::facet_wrap()` in order to show the same plot at various `n`
values.

```{r plot_vary_n, fig.asp = 1.2, fig.width = 4}
ggplot(vary_n_analytical) +
  geom_density(aes(x = estimate, color = method), linewidth = 1) +
  geom_vline(xintercept = 0.5) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    legend.title = element_blank()
  ) +
  xlim(0, 3) +
  xlab("Net growth rate estimate (r)") +
  ylab("Density") +
  scale_color_manual(
    labels = c("Internal lengths", "Max. likelihood"),
    values = c("black", colorPal[4])
  ) +
  facet_wrap(~ factor(paste0("n = ", n), levels = paste0("n = ", unique(n_vec))),
             ncol = 1, strip.position = "bottom", scales = "free", dir = "v"
  )
```

As you can see, both estimates are highly dependent on the number of
samples. If you'd like to see the same density values on the y-axis for
all `n` values, set `scales = "fixed"` in the `facet_wrap()` function.

### Markov Chain Monte Carlo (MCMC) application

Note: computationally intensive! Recommended to run in high performance
computing environment.

In our paper, we show two alternatives, each with their own set of
advantages and drawbacks. The drawback for both MCMC approaches is that
the computational expense is greatly increased compared to our
`internalLengths()` and `maxLikelihood()` estimates. Phylofit has the
additional drawback of overly confident confidence intervals, where the
95% Highest Posterior Density (HPD) doesn't capture the true value 95%
of the time. This is because the deterministic approximation of the
population size trajectory in Phylofit doesn't match the stochastic
underlying birth-death process which generates the simulated trees. The
`birthDeathMCMC()` approach doesn't suffer from this drawback.

With both MCMC approaches, the added computational expense comes with a
notable benefit. These methods do not require that the birth-death
process is supercritical, and will work well even for small growth
rates. We show this in the section [Varying r (and/or
cloneAge)](#varying-r-andor-cloneage). While Phylofit does have an
additional drawback, it also has increased flexibility. Phylofit will
work well in estimating the growth rate even if the coalescence times
are characteristic of a logistic clonal growth trajectory. In most
cases, the number of samples will be small enough and the effective
population size large enough that the coalescence times date back to the
exponential phase, before carrying capacity and logistic growth (or
other saturating growth) affect the coalescence times, in which case,
constant birth-death approximations (`birthDeathMCMC()`,
`maxLikelihood()`, and `internalLengths()`) work well.

### Summary statistics

We can write a basic function to give us all the summary statistics we
want from the estimates.

```{r summaryStats}
get_summary_stats <- function(df, growthRate){
  
}
```

This information is also available in the Supplementary tables, which
are also available as .csv files.

## Varying r (and/or cloneAge) {#varying-r-andor-cloneage}

As we noted before, we can simulate many trees at once using
`simUltra()` and `simMut()`. Here, we'll use `simUltra()` to generate
100 trees at various growth rates. We do a similar analysis in Figures 3
and 4 of [our
preprint](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/),
noting that our methods struggle with small growth rates. Let's see if
we come to the same conclusion here.

### Varying r or cloneAge?

First, we have to address something that might otherwise lead to
confusion; varying `r` is the same as varying `cloneAge`. If I simulate a population with a net growth
rate of 1 per year for 20 years it should look the same as a growth rate
of 0.5 per year for 40 years. We'll show this, but first consider the
fact that the units are meaningless, so long as they're consistent. So a
growth rate of 1/12 and a clone age of 20\*12 is exactly the same as a
growth rate of 1 and 20 if the units change from years to months.

This all might be a bit confusing, so let's plot it. First, we'll
simulate a tree with a growth rate of 2 for 30 years and compare it to a
tree with a growth rate of 0.5 and the same 30 years. These trees should
look different. Finally, we'll add a tree that has a growth rate of 0.5,
but run it for 120 years. If what I said above is true, we should see
that the last tree looks like the first tree, because 0.5 for 120 should
be the same as 2 for 30. Let's find out:

```{r sim_vary_r}
# First tree, r = a - b = 2
tree1 <- simUltra(a = 2.5, b = .5, cloneAge = 30, n = 50)

# Second tree, with r = a - b = 0.5
tree2 <- simUltra(a = 1, b = .5, cloneAge = 30, n = 50)

# Third tree, with r = 0.5 but cloneAge = 120
tree3 <- simUltra(a = 1, b = .5, cloneAge = 120, n = 50)
```

Now, let's plot using `par()` to show all three in one plot

```{r plot_vary_r, fig.asp = 0.4, fig.width = 5}
par(mfrow = c(1, 3))
ape::plot.phylo(tree1, direction = "downwards", show.tip.label = F, main = "r = 2, cloneAge = 30")
ape::plot.phylo(tree2, direction = "downwards", show.tip.label = F, main = "r = 0.5, cloneAge = 30")
ape::plot.phylo(tree3, direction = "downwards", show.tip.label = F, main = "r = 0.5, cloneAge = 120")
```

While the trees on the left and the right aren't identical due to the
stochastic nature of the birth-death process, they are similar, and
certainly different from the tree in the middle.

### Performance across r values

Now we can arbitrarily choose whether to vary `r` or `cloneAge`. For
consistency, we'll vary `r`, as we do in Figures 3 and 4 of [our
preprint](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/).
Here, we have two options:

1.  Simulate at fixed `r` values, showing which are good and which are
    problematic
2.  Simulate at random `r` values and then try to decipher which
    estimates are good and which aren't

Option 1 is essentially the same as we did in [Varying n], but with `r`
instead of `n`, so repeating that here wouldn't be as useful. Option 2
is also more realistic...we can pretend we don't know the ground truth
and show how we decide if our methods are relevant before comparing to
the ground truth.

Let's simulate 200 trees with 50 samples and various growth rates,
ranging from 0.1 to 1 per year, run for 40 years.

```{r vary_r}
# Uniform ditribution of r used to generate b_vec
r_vec <- stats::runif(n = 200, min = 0.1, max = 1)
a_vec <- stats::runif(n = 200, min = 1, max = 3)
b_vec <- a_vec - r_vec

# Input to simUltra()
vary_r_trees <- simUltra(a = a_vec, b = b_vec, cloneAge = 40, n = 50, nTrees = length(a_vec))
```

Apply our `maxLikelihood()` function as usual...

```{r maxLike_vary_r}
maxLike_vary_r <- maxLikelihood(vary_r_trees)
```

Uh oh... that's a long list of warnings. Let's read through and see
what's going on. The ratio of external to internal lengths is a measure
of how "supercritical" or star-shaped the birth-death process is. If the
internal lengths are at the top of the tree, we consider it star-shaped,
and it's a good candidate for our methods, which operate under the
assumption of a supercritical process. This means that the growth rate
is high enough, and enough time has passed, for the coalescence events
to occur well before the sampling time. In practice, using these
simulations, we show that an external to internal lengths ratio greater
than 3 is good enough to apply our methods. Let's apply our methods and
see how they look before and after applying this ratio cutoff. Note that
this is what we're doing in Figure 4 of [our
preprint](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/).

Let's apply the internal lengths method, this time suppressing the
warning.

```{r lengths_vary_r}
lengths_vary_r <- suppressWarnings(internalLengths(vary_r_trees))
```

Because we have different `r` values, we can't do a density plot as
before. We'll have to show a density plot with the x axis representing
percent error. In this case, good estimates will consolidate around 0.
First, we'll calculate error using the `estimate` column and `r_vec`
which has our ground truth values.

```{r coverage_vary_r, fig.asp = 0.8, fig.width = 7}
# Calculate error in lengths, adding columns
lengths_vary_r$ground_truth <- r_vec
lengths_vary_r$percent_error <- 100 * (lengths_vary_r$estimate - lengths_vary_r$ground_truth) / lengths_vary_r$ground_truth

# Do the same for maxLike
maxLike_vary_r$ground_truth <- r_vec
maxLike_vary_r$percent_error <- 100 * (maxLike_vary_r$estimate - maxLike_vary_r$ground_truth) / maxLike_vary_r$ground_truth

# Combine for ggplot formatting
results_vary_r <- rbind(lengths_vary_r, maxLike_vary_r)

# Plot, adding a vertical line at 0 because that's the error free estimate
ggplot(results_vary_r) +
  geom_density(aes(x = percent_error, color = method), linewidth = 1.5) +
  geom_vline(xintercept = 0) +
  theme_bw() +
  xlim(-100, 300) +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    legend.title = element_blank()
  ) +
  xlab("Estimate percent error") +
  ylab("Density") +
  scale_color_manual(labels = c("Internal lengths", "Max. likelihood"), values = c("black", "#009E73"))
```

Not bad, but we have some rogue estimates with \> 200% error! And we
have many with \> 50% error. Let's apply that cutoff ratio of 3 to
exclude the trees which we don't think are good candidates for our
methods. Hopefully, this will get rid of the very poor estimates.

```{r applyCutoff, fig.asp = 0.8, fig.width = 7}

# Cut those trees with ratio > 3
results_vary_r_cut <- results_vary_r[results_vary_r$extIntRatio >= 3, ]

# How many did we cut
print((nrow(results_vary_r) - nrow(results_vary_r_cut)) / 2)

# Plot, adding a vertical line at 0 because that's the error free estimate
ggplot(results_vary_r_cut) +
  geom_density(aes(x = percent_error, color = method), linewidth = 1.5) +
  geom_vline(xintercept = 0) +
  theme_bw() +
  xlim(-100, 300) +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    legend.title = element_blank()
  ) +
  xlab("Estimate percent error") +
  ylab("Density") +
  scale_color_manual(labels = c("Internal lengths", "Max. likelihood"), values = c("black", "#009E73"))
```

I'd say that's much better. We cut 24 of the 200 trees, eliminating many
of the worst estimates. The real advantage here is that we can look at a
tree calculate this ratio which will tell us right away if our methods
are applicable. Again, if you're interested in the quantitative side of
this, you can apply the metrics from the [Quantitative comparisons]
section.

# References

The method for simulating the trees is a direct result of the work of
Amaury Lambert in the following paper. The mathematical methods for
estimating growth rates build in large part from the same work, linked
below:

-   [Lambert, 2018](https://pubmed.ncbi.nlm.nih.gov/29704514/)

And here's a final link to [our
paper](https://www.biorxiv.org/content/10.1101/2023.02.24.529817v2/) for
more of the details of the methods and data analysis.

There aren't too many colors in this vignette, but we tried to use
colorblind friendly colors, specifically pulling colors from a palette
designed by [Bang Wong](https://www.nature.com/articles/nmeth.1618) and
available
[here.](https://davidmathlogic.com/colorblind/#%23000000-%23E69F00-%2356B4E9-%23009E73-%23F0E442-%230072B2-%23D55E00-%23CC79A7)
